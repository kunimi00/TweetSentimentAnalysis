{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python requistes\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Graph / Visualization\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "import pygraphviz\n",
    "\n",
    "\n",
    "## Tweet preprocessor\n",
    "import preprocessor as p\n",
    "\n",
    "## NLTK tokenization / lemmatization\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import treebank\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117659/117659 [00:01<00:00, 82730.46it/s]\n"
     ]
    }
   ],
   "source": [
    "synset_list = list(wn.all_synsets())\n",
    "wordnet_graph_synset = nx.Graph(engine='sfdp', pack=True)\n",
    "\n",
    "seen = set()\n",
    "for ss in tqdm(synset_list):\n",
    "    wordnet_graph_synset.add_node(ss.name())\n",
    "    for lm in ss.lemmas():\n",
    "        _lm = lm.name()\n",
    "        if not _lm in seen:\n",
    "            seen.add(_lm)\n",
    "            wordnet_graph_synset.add_node(_lm)\n",
    "        wordnet_graph_synset.add_edge(_lm, ss.name())\n",
    "\n",
    "path = './wordnet_graph_synset.p'\n",
    "nx.write_gpickle(wordnet_graph_synset, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117659/117659 [00:00<00:00, 357176.80it/s]\n",
      "100%|██████████| 117659/117659 [00:02<00:00, 57002.10it/s]\n"
     ]
    }
   ],
   "source": [
    "wordnet_graph_synset_cleaned = nx.Graph(engine='sfdp', pack=True)\n",
    "\n",
    "for ss in tqdm(synset_list):\n",
    "    wordnet_graph_synset_cleaned.add_node(ss.name())\n",
    "\n",
    "for ss in tqdm(synset_list):\n",
    "    nb_list = [k for m in [n for n in wordnet_graph_synset.neighbors(ss.name())] for k in wordnet_graph_synset.neighbors(m)]    \n",
    "    for nb in nb_list:\n",
    "        if wordnet_graph_synset_cleaned.has_edge(ss.name(), nb) == False:\n",
    "            wordnet_graph_synset_cleaned.add_edge(ss.name(), nb)\n",
    "\n",
    "path = './wordnet_graph_synset_cleaned.p'\n",
    "nx.write_gpickle(wordnet_graph_synset_cleaned, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wn.synset('amazing.s.02').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import treebank\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inspiring awe or admiration or wonder'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inspiring', 'awe', 'or', 'admiration', 'or', 'wonder']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inspiring', 'awe', 'or', 'admiration', 'or', 'wonder']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb_tkn = treebank.TreebankWordTokenizer()\n",
    "tb_tkn.tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inspiring', 'awe', 'or', 'admiration', 'or', 'wonder']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_tkn = TweetTokenizer()\n",
    "tw_tkn.tokenize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation\n",
    "\n",
    "replace_dict = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "}\n",
    "\n",
    "def replace_word(text):\n",
    "    for word in replace_dict:\n",
    "        if word in text:  # Small first letter\n",
    "            text = text.replace(word, replace_dict[word])\n",
    "        elif word[0].title() + word[1:] in text:  # Big first letter\n",
    "            text = text.replace(word[0].title() + word[1:],\n",
    "                                replace_dict[word][0].title() + replace_dict[word][1:])\n",
    "\n",
    "    return text\n",
    "\n",
    "def neg_tagging(word_list):\n",
    "    string = ' '.join(word_list)\n",
    "    transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s]+[^\\w\\s]', \n",
    "           lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
    "           string,\n",
    "           flags=re.IGNORECASE)\n",
    "    \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(word_list):\n",
    "    negged_sentence = neg_tagging(word_list)\n",
    "    negged_tokens = negged_sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    token_pair_list = []\n",
    "    \n",
    "    for word in negged_tokens:\n",
    "        negation = False\n",
    "        if word.startswith('NEG_'):\n",
    "            negation = True\n",
    "            word = word[4:]\n",
    "        token_pair_list.append((word, negation))\n",
    "        tokens.append(word)\n",
    "    \n",
    "    return tokens, token_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tk = word_tokenize(replace_word(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inspiring', 'awe', 'or', 'admiration', 'or', 'wonder']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk, original_lesk, cosine_lesk, adapted_lesk\n",
    "from pywsd import disambiguate\n",
    "from pywsd.similarity import max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = disambiguate(df, cosine_lesk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inspiring', Synset('inspire.v.01')),\n",
       " ('awe', Synset('awe.n.01')),\n",
       " ('or', None),\n",
       " ('admiration', Synset('admiration.n.01')),\n",
       " ('or', None),\n",
       " ('wonder', Synset('wonder.n.01'))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_list = list(wn.all_synsets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better WSD library\n",
    "from pywsd.lesk import simple_lesk, original_lesk, cosine_lesk, adapted_lesk\n",
    "from pywsd import disambiguate\n",
    "from pywsd.similarity import max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 29110/117659 [43:31<2:12:24, 11.15it/s]"
     ]
    }
   ],
   "source": [
    "all_pairs_from_definition = []\n",
    "for ss in tqdm(synset_list):\n",
    "    df = ss.definition()\n",
    "    df_pair = disambiguate(df, cosine_lesk)\n",
    "    all_pairs_from_definition.extend(df_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_pairs_from_definition = []\n",
    "for ss in tqdm([wn.synset('amazing.s.02'), wn.synset('good.a.01')]):\n",
    "    df = ss.definition()\n",
    "    curr_df_pair_list = disambiguate(df, cosine_lesk)\n",
    "    df_pair_txt_list = []\n",
    "    for curr_df_pair in curr_df_pair_list:\n",
    "        if curr_df_pair[1] is None:\n",
    "            df_pair_txt_list.append(curr_df_pair)\n",
    "        else:\n",
    "            df_pair_txt_list.append((curr_df_pair[0], curr_df_pair[1].name()))\n",
    "    all_pairs_from_definition.append((ss.name(), df_pair_txt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing.s.02',\n",
       "  [('inspiring', 'inspire.v.01'),\n",
       "   ('awe', 'awe.n.01'),\n",
       "   ('or', None),\n",
       "   ('admiration', 'admiration.n.01'),\n",
       "   ('or', None),\n",
       "   ('wonder', 'wonder.n.01')]),\n",
       " ('good.a.01',\n",
       "  [('having', None),\n",
       "   ('desirable', 'desirable.a.01'),\n",
       "   ('or', None),\n",
       "   ('positive', 'positive.a.01'),\n",
       "   ('qualities', 'quality.n.01'),\n",
       "   ('especially', 'particularly.r.01'),\n",
       "   ('those', None),\n",
       "   ('suitable', None),\n",
       "   ('for', None),\n",
       "   ('a', None),\n",
       "   ('thing', 'thing.n.01'),\n",
       "   ('specified', 'stipulate.v.01')])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pairs_from_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_pairs_from_definition, open( \"all_pairs_from_definition.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
